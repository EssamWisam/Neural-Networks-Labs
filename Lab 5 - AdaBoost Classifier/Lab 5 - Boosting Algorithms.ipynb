{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5 - Classifiers Boosting Algorithms\n",
    "\n",
    "In this lab, we will implement the AdaBoost algorithm as an ensemble learning technique which\n",
    "aims to combine a number of weak classifiers to yield a strong classifier at the end.\n",
    "The idea of this lab is to identify whether a tumor with given characteristics is malignant or\n",
    "benign. This is a two-class classification problem.\n",
    "\n",
    "## Dataset and Features\n",
    "\n",
    "You will be working on the dataset from *Hastie et al,* for breast tumor classification with 10 features representing the tumor's:\n",
    "\n",
    "                              1. Area            6. Texture\n",
    "                              2. Perimeter       7. Symmetry\n",
    "                              3. Radius          8. Greyscale Level\n",
    "                              4. Compactness     9. Fractal Dimension\n",
    "                              5. Concavity      10. Coastline Approximation.\n",
    "There is one output variable which is diagnosis. It takes one of two values `+1` for malignant and `-1` for benign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "Why it is sometimes better to have the two class values `+1` and `-1` instead of `+1`\n",
    "and `0`?\\\n",
    "**HINT :** Think about the voting scheme at the end of the boosting algorithm. How can the class values\n",
    "affect this scheme?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Your answer: \\n    The derivation of Adaboost as a forward stage-wise additive model starts by assuming that the classifiers\\n    will give -1, 1 (by considering the exponential loss function.) Nonetheless, with the algorithm derivied the\\n    only thing that depends on the the class labes is the voting scheme. \\n\\n    The original voting scheme uses the sign of the summation to decide whether the alphas for the weak classifiers\\n    voting for the first class is greater than the sum of the alphas of the weak classifiers voting for the second\\n    class. In the slides however we covered a different formulation that checks for the same thing regardless to what\\n    labels we use for the classes.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Your answer: \n",
    "    The derivation of Adaboost as a forward stage-wise additive model starts by assuming that the classifiers\n",
    "    will give -1, 1 (by considering the exponential loss function.) Nonetheless, with the algorithm derivied the\n",
    "    only thing that depends on the the class labes is the voting scheme. \n",
    "\n",
    "    The original voting scheme uses the sign of the summation to decide whether the alphas for the weak classifiers\n",
    "    voting for the first class is greater than the sum of the alphas of the weak classifiers voting for the second\n",
    "    class. In the slides however we covered a different formulation that checks for the same thing regardless to what\n",
    "    labels we use for the classes.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirement\n",
    "You are required to fill the function `adaboost_classifier(Y_train, X_train, Y_test, X_test, T, clf).`\\\n",
    "This function takes as parameters:\n",
    "\n",
    "| | |\n",
    "|:---|:-|\n",
    "| **Y_train**| The target values for the training set |\n",
    "| **X_train**| The input features for the training set.|\n",
    "| **Y_test**| The target values for the test set.|\n",
    "| **X_test**| The input features for the testing set.|\n",
    "| **T**| The number of iterations of the AdaBoost Algorithm.|\n",
    "| **clf**| The classifier to be used. (In our case, we are using a decision tree stump as a base classifier). You can use any other classifier.|\n",
    "\n",
    "This function should return two values:\n",
    "- The accuracy of the model on the training set.\n",
    "- The accuracy of the model on the test set.\n",
    "\n",
    "\n",
    "#### Fair Note:\n",
    "In the explanation video, we assumed that (T) is the number of models you want to fit. However, this is not always the case. You may have a model base (like here we have decision trees) and you are allowed to use as many of it as you can. So (T) here becomes the number of iterations where your goal is to enhance the performance with as few iterations as possible. \n",
    "\n",
    "Do not get confused:\n",
    "- If your case is you have T models only, we set T = number of models to fit.\n",
    "- If you are allowed to use as many models as you can (as many decision trees as you need), then T is the number of iterations to choose. In such case, T becomes a parameter controlled by the programmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports ##\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "\n",
    "from utils import get_accuracy, get_error_rate, print_accuracy, plot_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** we prepared some utility functions to help you throughout the lab. please go and check the file *utils.py* and make sure you understand each function and know how to use it.\n",
    "\n",
    "### DONE: AdaBoost Implementation\n",
    "\n",
    "AdaBoost is an iterative algorithm that gives weights for the best classifier every iteration, updates weights of the data points, then repeats until convergence.\n",
    "\n",
    "The steps of the algorithm are:\n",
    "\n",
    "1. Initialize weights of the training examples:\n",
    "\n",
    "$$w_{m} = \\frac {1}{M}, m = 1,2,...M$$\n",
    "\n",
    "                                        M: number of training examples. \n",
    "\n",
    "2. For t=1 to $T$:\n",
    "\n",
    "    a) Select a classifier $h_{t}$ that best fits to the training data using weights $w_{m}$ of the training examples.\n",
    "\n",
    "    b) Compute error of $h_{t}$ as:\n",
    "$$err_{t} = \\frac {\\Sigma_{m=1}^{M} w_{m} \\phi (c_{m} \\neq h_{t}(x_{m}))}{\\Sigma_{m=1}^{M} w_{m}}$$\n",
    "\n",
    "    c) Compute weight of classifier:\n",
    "$$\\alpha_{t} = \\log (\\frac {1-err_{t}}{err_{t}} )$$\n",
    "\n",
    "    d) Update weights of wrongly classified examples:\n",
    "$$w_{m} = w_{m} * \\exp^{\\alpha_{t} \\phi (c_{m} \\neq h_{t}(x_{m}))}, \\space m = 1 ... M$$\n",
    "\n",
    "    e) Renormalize weights $w_{m}$\n",
    "\n",
    "\n",
    "\\\n",
    "3. Output: $C(x)= argmax_{k}\\space (\\space \\Sigma_{t=1}^{T} \\alpha_{t} * \\phi (h_{t}(x) = k)) \\space)$\n",
    "\n",
    "An interesting formula, perhaps. Suppose we have two classes (k=2), the class is either the first or the second depending on which yields the greater sum (in each sum we only look at the classifiers that agree with the result.)\n",
    "\n",
    "\n",
    "**Where** in step 2.B and 2.D, the $\\phi (y)$ function is called the *miss indicator* function that gives values:\n",
    "\n",
    "                                     1: if y is True\n",
    "                                     0: if y is False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost_classifier(Y_train, X_train, Y_test, X_test, T, clf):      \n",
    "    # T is the no. of iterations.\n",
    "    # clf is the weak classifier\n",
    "    \n",
    "    #DONE: FILL THE FUNCTION with the implementation as the steps above\n",
    "\n",
    "    # DONE [1]: Initialize weights      \n",
    "    w = np.array([1/len(X_train) for x in X_train ])            # for e\n",
    "\n",
    "    ## DONE [2]:  Initialize the training and test predicted classes with empty array placeholders\n",
    "    #### Hint: what should be their shape?      ## Like Y_train and Y_test respectively\n",
    "    pred_train = np.zeros((T,len(Y_train) ))         ## predicted classes of the training examples\n",
    "    pred_test = np.zeros((T,len(Y_test) ))          ## predicted classes of the test examples\n",
    "\n",
    "    ## DONE [3]: loop over the boosting iterations \n",
    "    for t in range(T): \n",
    "\n",
    "        # DONE [4]: Fit a classifier with the specific weights \n",
    "        ## DONE [4.A]: fit the classifier on the training data\n",
    "        #### Hint: search how sklearn.tree.DecisionTreeClassifier fits classifier on data\n",
    "        ### Hint: search for parameter weights in the fit matrix\n",
    "        clf = clf.fit(X_train,Y_train, w)\n",
    "        \n",
    "        # DONE [4.B]: predict classes for the training data and test data\n",
    "        pred_train_t = clf.predict(X_train)\n",
    "        pred_test_t = clf.predict(X_test)\n",
    "        \n",
    "        # DONE [5]: calculate the miss Indicator function\n",
    "        I = (pred_train_t != Y_train)             #Assuming Y_test is a numpy array like pred_test_t\n",
    "        \n",
    "        # DONE [6]: calculate the error for the current classifier (err_t)\n",
    "        err_t = np.sum(w * I)/np.sum(w)\n",
    "        \n",
    "        # DONE [7]: calculate current classifier weight (Alpha_t)\n",
    "        alpha_t = np.log((1-err_t)/err_t)\n",
    "        \n",
    "        # DONE [8]: update the weights \n",
    "        w = w * np.exp(alpha_t * I)             # a lot of cool vectorization is going on here.\n",
    "        \n",
    "        # DONE [9] Add to the overall predictions\n",
    "        pred_train[t] = pred_train_t * alpha_t\n",
    "        pred_test[t] = pred_test_t * alpha_t\n",
    "\n",
    "    # Now pred_train is an array of t rows with predictions scaled by alpha as the formula suggested.\n",
    "    # Need to add all these and come up with one row for the classification results\n",
    "    # We will use the voting scheme that relies on the sign since classes are -1 and +1\n",
    "\n",
    "    final_pred_train = np.sum(pred_train, axis=0)\n",
    "    final_pred_test = np.sum(pred_test, axis=0)\n",
    "\n",
    "    # apply the sign function on all of them:\n",
    "    final_pred_train[final_pred_train >= 0], final_pred_train[final_pred_train < 0]= 1, -1\n",
    "    final_pred_test[final_pred_test >= 0], final_pred_test[final_pred_test < 0]= 1, -1\n",
    "\n",
    "\n",
    "    # DONE [10]: Return error rate in train and test set\n",
    "    #### Hint: use function get_accuracy from utils.py\n",
    "    train_acc = get_accuracy(final_pred_train, Y_train )\n",
    "    test_acc = get_accuracy(final_pred_test, Y_test )\n",
    "    print_accuracy(train_acc, test_acc)\n",
    "    return train_acc, test_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Boosted Classifier\n",
    "\n",
    "Now we will use the function you implemented to build a classifer.\\\n",
    "You will not change code here, only read the code below and run it to see how **AdaBoost** enhanced the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data ...\n",
      "Number of Iterations :  10\n",
      "Accuracy: Training: 66.96875% - Test: 66.33333333333333%\n",
      "Number of Iterations :  60\n",
      "Accuracy: Training: 83.11458333333334% - Test: 82.16666666666667%\n",
      "Number of Iterations :  110\n",
      "Accuracy: Training: 86.75% - Test: 84.375%\n",
      "Number of Iterations :  160\n",
      "Accuracy: Training: 88.55208333333333% - Test: 87.04166666666666%\n",
      "Number of Iterations :  210\n",
      "Accuracy: Training: 89.84375% - Test: 88.625%\n",
      "Number of Iterations :  260\n",
      "Accuracy: Training: 90.10416666666666% - Test: 88.0%\n",
      "Number of Iterations :  310\n",
      "Accuracy: Training: 91.75% - Test: 89.83333333333333%\n",
      "Number of Iterations :  360\n",
      "Accuracy: Training: 92.05208333333333% - Test: 90.70833333333333%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/essam/Documents/GitHub/ML-Labs/Lab 5 - AdaBoost Classifier/utils.py:23: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  plot1.set_xticklabels(range(0, 450, 50))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### DO NOT CHANGE CODE ####\n",
    "\n",
    "## First, read the dataset\n",
    "x,y = make_hastie_10_2()\n",
    "df = pd.DataFrame(x)\n",
    "df['Y'] = y\n",
    "print('Reading Data ...')\n",
    "\n",
    "# Split into training and test set\n",
    "train, test = train_test_split(df, test_size=0.2) # this function shuffles the data points, and splits the data into\n",
    "                                                  # 80% training set and 20% test set (indicated by test_size=0.2)\n",
    "\n",
    "train, test = np.array(train), np.array(test)\n",
    "# For some reason pandas loops on columns when doing list comprehension unless [0] is specified.\n",
    "# Not cool --> switch to Numpy and even get rid of iloc.\n",
    "\n",
    "# print(train.shape) (9600, 11) 10 features and last column is for class\n",
    "# print(train.iloc[1:10, 10])   it takes values -1, 1\n",
    "\n",
    "X_train, Y_train = train[:, :-1], train[:, -1]            # last column should to go to classes (Y_train)\n",
    "X_test, Y_test = test[:, :-1], test[:, -1]                # last column should go to classes (Y_test)\n",
    "\n",
    "# X_Train is now of dimensions (9600, 10) and Y_train is of dimensions (9600,) for test sets its 2400 rows (20%)\n",
    "\n",
    "# Fit a simple decision tree first\n",
    "clf_tree = DecisionTreeClassifier(max_depth=1, random_state=1)      \n",
    "# random_state: a sub-optimal greedy algorithm is repeated a number of times using random selections of features and samples\n",
    "# here we give it a seed of 1 so results are always consistent.\n",
    "# max_depth = 1 means it's always one split. (very week classifier)\n",
    "\n",
    "# Fit Adaboost classifier using a decision tree as base estimator\n",
    "# Test with different number of iterations\n",
    "acc_train, acc_test = [],[]\n",
    "x_range = range(10, 410, 50)\n",
    "for i in x_range:\n",
    "    print('Number of Iterations : ' , i)\n",
    "    acc_i = adaboost_classifier(Y_train, X_train, Y_test, X_test, i, clf_tree)\n",
    "    acc_train.append(acc_i[0])\n",
    "    acc_test.append(acc_i[1])\n",
    "\n",
    "# Compare error rate vs number of iterations\n",
    "plot_accuracy(acc_train, acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "Justify why the plot is the way it is (is it increasing or decreasing? why? when does it flattens out?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Your answer:\\n    It\\'s increasing. There is a drastic decrease in slope starting from 100 iterations.\\n    Going further the slope keeps decreasing until it becomes about 0 starting form 350 iterations.\\n\\n    The accuracy keeps on improving with iterations because in each iteration the classifier learns from its mistakes\\n    in the previous iteration  and is then weighted depending on the new resulting loss it computes (the smaller it is\\n    the larger the weight) hence, accuracy should naturally increase with iterations (every iteration is more learning.)\\n\\n    Now as we can see, at some point it starts to decrease then flatten out which means that there is less to learn\\n    every iteration (at the beginning with 65 percent accuracy it had much to learn.) so its also natural for the \\n    accuracy improvement to slow down as we go. \\n\\n    Side note:\\n    One would assume that more iterations may lead to overfitting (it turns out to be more robust than expected):\\n    \"As shown in L2Boost by Peter Bühlmann, as the number of weak learners (rounds of boosting) increases, \\n    the bias converges exponentially fast while the variance increases by geometrically diminishing magnitudes \\n    which means: It overfits much slower than most of the other methods.\"\\n    https://stats.stackexchange.com/questions/20622/is-adaboost-less-or-more-prone-to-overfitting\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Your answer:\n",
    "    It's increasing. There is a drastic decrease in slope starting from 100 iterations.\n",
    "    Going further the slope keeps decreasing until it becomes about 0 starting form 350 iterations.\n",
    "\n",
    "    The accuracy keeps on improving with iterations because in each iteration the classifier learns from its mistakes\n",
    "    in the previous iteration  and is then weighted depending on the new resulting loss it computes (the smaller it is\n",
    "    the larger the weight) hence, accuracy should naturally increase with iterations (every iteration is more learning.)\n",
    "\n",
    "    Now as we can see, at some point it starts to decrease then flatten out which means that there is less to learn\n",
    "    every iteration (at the beginning with 65 percent accuracy it had much to learn.) so its also natural for the \n",
    "    accuracy improvement to slow down as we go. \n",
    "\n",
    "    Side note:\n",
    "    One would assume that more iterations may lead to overfitting (it turns out to be more robust than expected):\n",
    "    \"As shown in L2Boost by Peter Bühlmann, as the number of weak learners (rounds of boosting) increases, \n",
    "    the bias converges exponentially fast while the variance increases by geometrically diminishing magnitudes \n",
    "    which means: It overfits much slower than most of the other methods.\"\n",
    "    https://stats.stackexchange.com/questions/20622/is-adaboost-less-or-more-prone-to-overfitting\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "The number of iterations (T) is what we call a hyper parameter:\n",
    "   - Its value differs from model to model and from problem to problem.\n",
    "   - Its value is not learnt by time, it is set by the programmer.\n",
    "   \n",
    "Suggest ways to select the optimal T keeping in mind that:\n",
    "   - If T is too big, the training time is large (you loop for T times, each time takes a model to fit and this model might take hours to fit)\n",
    "   - If T is too small, the boosting might not reach the best values it can get.\n",
    "   \n",
    "   \n",
    "\n",
    "**HINT**: Look at the graph of number of iterations vs performance and search for elbow method. Try to understand it and explain what it does.\\\n",
    "**HINT**: There are other hyper-parameter selection techniques, search for them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Your answer:\\n    https://stats.stackexchange.com/questions/163560/how-many-adaboost-iterations\\n    \"Mease and Wyner (2008) argue that AdaBoost should be run for a long time, until it converges, \\n    and that 1,000 iterations should be enough.\"\\n    So assuming speed is not issue we can run it a large number of iterations without worrying (e.g. 1K as by Mease et al)    \\n    \\n    The elbow method is often used to select the number of clusters in k-means, we keep watching the loss function\\n    as it decreases with the number of clusters and stop at the \\'elbow of the curve\\'. We can apply the same huerstic\\n    to find the number of iterations here. The only different is the the elbow here is more like a \\'knee\\' (since \\n    we\\'re plotting accuracy.) stopping there makes sense because we don\\'t get so much from continuing and we make\\n    the model (even if its a special case here) more susceptible to overfitting all while paying the same amount\\n    of resources (computationa and time.)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Your answer:\n",
    "    https://stats.stackexchange.com/questions/163560/how-many-adaboost-iterations\n",
    "    \"Mease and Wyner (2008) argue that AdaBoost should be run for a long time, until it converges, \n",
    "    and that 1,000 iterations should be enough.\"\n",
    "    So assuming speed is not issue we can run it a large number of iterations without worrying (e.g. 1K as by Mease et al)    \n",
    "    \n",
    "    The elbow method is often used to select the number of clusters in k-means, we keep watching the loss function\n",
    "    as it decreases with the number of clusters and stop at the 'elbow of the curve' (knee here). We can apply the same huerstic\n",
    "    to find the number of iterations here.  Stopping there makes sense because we don't get so much from continuing and we make\n",
    "    the model (even if its a special case here) more susceptible to overfitting all while paying the same amount\n",
    "    of resources (computationa and time.)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
